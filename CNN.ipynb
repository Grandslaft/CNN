{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execution time wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_execution_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time of {func.__name__}: {end_time - start_time} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class for the Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, name, alpha=1):\n",
    "        self.name = name\n",
    "        if name == 'sigmoid':\n",
    "            self.function = self.exponent\n",
    "            self.derivative = self.exponent_der\n",
    "        elif name == 'tanh':\n",
    "            self.function = lambda x: np.tanh(x)\n",
    "            self.derivative = lambda x: 1 - np.square(np.tanh(x))\n",
    "        elif name == 'softmax':\n",
    "            self.function = self.softmax\n",
    "            self.derivative = self.softmax_der\n",
    "        elif name == 'relu':\n",
    "            self.function = lambda x: np.maximum(0, x)\n",
    "            self.derivative = lambda x: x >= 0\n",
    "        elif name == 'lrelu':\n",
    "            self.function = lambda x: np.maximum(0.0001, x) \n",
    "            self.derivative = lambda x: np.where(x >= 0, 1, 0.0001)\n",
    "        elif name == 'elu':\n",
    "            self.function = lambda x: np.where(x >= 0, x, alpha*(np.exp(x) - 1))\n",
    "            self.derivative = lambda x: np.where(x >= 0, 1, self.function(x) + alpha)\n",
    "        \n",
    "    def exponent(self, x):\n",
    "        shift_x = x - np.max(x)\n",
    "        return 1/(1 + np.exp(-shift_x))\n",
    "    \n",
    "    def exponent_der(self, x):\n",
    "        exps = self.exponent(x)\n",
    "        return exps * (1 - exps)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        shift_x = x - np.max(x)\n",
    "        exps = np.exp(shift_x)\n",
    "        return exps/np.sum(exps)\n",
    "    \n",
    "    def softmax_der(self, x):\n",
    "        array = self.softmax(x).reshape(-1,1)\n",
    "        return np.diagflat(array) - np.dot(array, array.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "\n",
    "    def __init__(self, num_filters = 3, size = 3, stride = 1, activation = 'relu'):\n",
    "        self.next_layer = None\n",
    "        self.num_filters = num_filters\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "        self.filters = np.random.randn(num_filters, size, size) / 10\n",
    "    \n",
    "    def add_layer(self, child):\n",
    "        self.next_layer = child\n",
    "        return self\n",
    "    \n",
    "    # @calculate_execution_time            \n",
    "    def forward(self, image):\n",
    "        self.last_input = image\n",
    "        input_dimension = image.shape[1]\n",
    "        output_dimension = int((input_dimension - self.size) / self.stride) + 1\n",
    "        output = np.zeros((self.filters.shape[0], output_dimension, output_dimension))\n",
    "        \n",
    "        for f in range(self.num_filters):\n",
    "            current_y = output_y = 0\n",
    "            \n",
    "            while current_y + self.size <= input_dimension:\n",
    "                current_x = output_x = 0\n",
    "                \n",
    "                while current_x + self.size <= input_dimension:\n",
    "                    patch = image[:, current_y:current_y + self.size, current_x:current_x + self.size]\n",
    "                    output[f, output_y, output_x] = np.sum(self.filters[f] * patch)\n",
    "                    current_x += self.stride\n",
    "                    output_x += 1\n",
    "                    \n",
    "                current_y += self.stride\n",
    "                output_y += 1\n",
    "        \n",
    "        output = self.last_output = self.activation.function(output)\n",
    "        return output\n",
    "    \n",
    "    # @calculate_execution_time\n",
    "    def backward(self, out_prev, lr):\n",
    "        out_prev = np.clip(out_prev, -1e+5, 1e+5)\n",
    "        input_dimension = self.last_input.shape[1]\n",
    "        \n",
    "        out_prev = out_prev * self.last_output\n",
    "        \n",
    "        out_next = np.zeros(self.last_input.shape)\n",
    "        dfilt = np.zeros(self.filters.shape)\n",
    "        \n",
    "        for f in range(self.filters.shape[0]):\n",
    "            current_y = output_y = 0\n",
    "            \n",
    "            while current_y + self.size <= input_dimension:\n",
    "                current_x = output_x = 0\n",
    "                \n",
    "                while current_x + self.size <= input_dimension:\n",
    "                    patch = self.last_input[:, current_y:current_y + self.size, current_x:current_x + self.size]\n",
    "                    dfilt[f] += np.sum(out_prev[f, output_y, output_x] * patch, axis=0)\n",
    "                    out_next[:, current_y:current_y + self.size, current_x:current_x + self.size] += out_prev[f, output_y, output_x] * self.filters[f]\n",
    "                    current_x += self.stride\n",
    "                    output_x += 1\n",
    "                    \n",
    "                current_y += self.stride\n",
    "                output_y += 1\n",
    "                \n",
    "        self.filters -= lr * dfilt\n",
    "        return out_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    \n",
    "    def __init__(self, size = 2, stride = 2, activation = 'relu'):\n",
    "        self.next_layer = None\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "        self.activation = Activation(activation)\n",
    "    \n",
    "    def add_layer(self, child):\n",
    "        self.next_layer = child\n",
    "        return self\n",
    "    \n",
    "    # @calculate_execution_time            \n",
    "    def forward(self, image):\n",
    "        self.last_input = image\n",
    "        \n",
    "        nfilt, h_prev, w_prev = image.shape\n",
    "        h = int((h_prev - self.size) / self.stride) + 1\n",
    "        w = int((w_prev - self.size) / self.stride) + 1\n",
    "        \n",
    "        downscaled = np.zeros((nfilt, h, w))\n",
    "        \n",
    "        for i in range(nfilt):\n",
    "            curr_y = output_y = 0\n",
    "            \n",
    "            while curr_y + self.size <= h_prev:\n",
    "                curr_x = output_x = 0\n",
    "                \n",
    "                while curr_x + self.size <= w_prev:\n",
    "                    patch = image[i, curr_y:curr_y + self.size, curr_x:curr_x + self.size]\n",
    "                    downscaled[i, output_y, output_x] = np.max(patch)\n",
    "                    curr_x += self.stride\n",
    "                    output_x += 1\n",
    "                    \n",
    "                curr_y += self.stride\n",
    "                output_y += 1\n",
    "                \n",
    "        return downscaled\n",
    "    # @calculate_execution_time\n",
    "    def backward(self, out_prev, learning_rate):\n",
    "        out_prev = np.clip(out_prev, -1e+5, 1e+5)\n",
    "        out_next = np.zeros(self.last_input.shape)\n",
    "\n",
    "        nfilt, shape, _ = self.last_input.shape\n",
    "        \n",
    "        for c in range(nfilt):\n",
    "            current_y = output_y = 0\n",
    "            while current_y + self.size <= shape:\n",
    "                current_x = output_x = 0\n",
    "                while current_x + self.size <= shape:\n",
    "                    patch = self.last_input[c, current_y:current_y + self.size, current_x:current_x + self.size]\n",
    "                    (x, y) = np.unravel_index(np.nanargmax(patch), patch.shape)\n",
    "                    out_next[c, current_y + x, current_x + y] += out_prev[c, output_y, output_x]\n",
    "                    current_x += self.stride\n",
    "                    output_x += 1\n",
    "                current_y += self.stride\n",
    "                output_y += 1\n",
    "        return out_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_inputs, n_neurons, activation):\n",
    "        self.next_layer = None\n",
    "        # self.weights = np.random.uniform(size=(n_inputs, n_neurons)) - 0.5\n",
    "        # self.biases = np.random.uniform(size=(n_neurons,)) - 0.5\n",
    "        \n",
    "        # self.weights = np.random.normal(loc=0, scale=np.sqrt(2/n_inputs), size=(n_inputs, n_neurons))\n",
    "        # self.biases = np.random.normal(loc=0, scale=np.sqrt(2/n_inputs), size=(n_neurons))\n",
    "        \n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) / 10\n",
    "        self.biases = np.random.randn(n_neurons) / 10\n",
    "        \n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "    def add_layer(self, child):\n",
    "        self.next_layer = child\n",
    "        return self\n",
    "    \n",
    "    # @calculate_execution_time\n",
    "    def forward(self, image):\n",
    "        self.last_input_shape = image.shape\n",
    "        image = image.flatten()\n",
    "        output = np.dot(image, self.weights) + self.biases\n",
    "        self.last_input = image\n",
    "        self.last_output = output\n",
    "        return self.activation.function(output)\n",
    "    \n",
    "    # @calculate_execution_time\n",
    "    def backward(self, out_prev, learning_rate):\n",
    "        out_prev = np.clip(out_prev, -1e+5, 1e+5)\n",
    "        \n",
    "        if self.next_layer is None:\n",
    "            dW = out_prev[:, np.newaxis]\n",
    "        else:\n",
    "            dW = out_prev[:, np.newaxis] * self.activation.derivative(self.last_output)[:, np.newaxis]\n",
    "        dW = (dW * self.last_input[np.newaxis, :]).T\n",
    "        db = np.copy(out_prev)\n",
    "        out_next = self.weights @ out_prev\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        return out_next.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        if len(layers) < 2:\n",
    "            raise ValueError(\"Not enough layers. Should be at least 2\")\n",
    "        \n",
    "        self.layers = [layers[0]]\n",
    "        \n",
    "        for layer in layers[1:]:\n",
    "            self.layers[-1].add_layer(layer)\n",
    "            if layer is not None and layer not in self.layers:\n",
    "                self.layers.append(layer)\n",
    "        \n",
    "        self.early_stop = None\n",
    "        \n",
    "    # @calculate_execution_time\n",
    "    def forward(self, image):\n",
    "        for layer in self.layers:\n",
    "            image = layer.forward(image)\n",
    "        return image\n",
    "    \n",
    "    # @calculate_execution_time\n",
    "    def backward(self, gradient, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient, learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val = None, y_val = None, epochs=10, learning_rate=0.1, lr_decay=0, step=50, val_step=100):\n",
    "        data_amount = X_train.shape[0]\n",
    "        num_digits = len(str(data_amount))\n",
    "        learning_rate_0 = learning_rate\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            accuracies = []\n",
    "            losses = []\n",
    "            \n",
    "            self.history['accuracy'][epoch] = []\n",
    "            self.history['loss'][epoch] = []\n",
    "                \n",
    "            self.history['val_accuracy'][epoch] = []\n",
    "            self.history['val_loss'][epoch] = []\n",
    "            \n",
    "            for i, X in enumerate(X_train):              \n",
    "                output = self.forward(X)\n",
    "                \n",
    "                accuracy = output[np.argmax(y_train[i])]\n",
    "                loss = -np.sum(y_train[i] * np.log(output))\n",
    "                accuracies.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                gradient = np.copy(output) - y_train[i]\n",
    "                self.backward(gradient, learning_rate)\n",
    "                \n",
    "                learning_rate = (1/(1+lr_decay*((epoch + 1) * (i + 1)))) * learning_rate_0\n",
    "                \n",
    "                if i > val_step and i % step == 0:\n",
    "                    avg_accuracy = np.mean(accuracies)\n",
    "                    avg_loss = np.mean(losses)\n",
    "                    \n",
    "                    accuracies = []\n",
    "                    losses = []\n",
    "                    \n",
    "                    if self.early_stop is not None:\n",
    "                        if self.EarlyStopping(epoch, i, avg_accuracy, avg_loss, step, val_step, X_val, y_val):\n",
    "                            self.output = np.array(output)\n",
    "                            return\n",
    "                    \n",
    "                    val_accuracy, val_loss = self.history['val_accuracy'][epoch][-1], self.history['val_loss'][epoch][-1]\n",
    "                    \n",
    "                    print(f'[Image {i:{num_digits}d}]: ' +\n",
    "                          f'Avg Acc: {avg_accuracy:7.2%} | ' +\n",
    "                          f'Avg Loss {avg_loss:5.2f} | ' +\n",
    "                          f'Val Acc: {val_accuracy:7.2%} | '+\n",
    "                          f'Val Loss: {val_loss:5.2f}')\n",
    "                          \n",
    "\n",
    "    def val_pred(self, X_val, y_val):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for i, X in enumerate(X_val):\n",
    "            current_output = self.forward(X)\n",
    "            \n",
    "            accuracy = current_output[np.argmax(y_val[i])]\n",
    "            loss = -np.sum(y_val[i] * np.log(current_output))\n",
    "            accuracies.append(accuracy)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        return np.mean(accuracies), np.mean(losses)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([np.argmax(self.forward(x), 0) for x in X])\n",
    "        \n",
    "    def EarlyStop(self, monitor=\"accuracy\", min_delta=.1, min_monitor = 0.6, patience=3, restore_best_layers=False):\n",
    "        self.early_stop = {\n",
    "            \"monitor\": monitor, \n",
    "            \"min_delta\": min_delta, \n",
    "            \"min_monitor\": min_monitor, \n",
    "            \"patience\": patience,\n",
    "            \"restore_best_layers\": restore_best_layers,\n",
    "        }\n",
    "        \n",
    "        self.history = {\n",
    "            \"accuracy\": {},\n",
    "            \"loss\": {},\n",
    "            'val_accuracy': {},\n",
    "            'val_loss': {},\n",
    "            \"global_max_index\": (0, 0),\n",
    "            \"best_layers\": []\n",
    "        }\n",
    "    \n",
    "    def EarlyStopping(self, epoch, im_i, accuracy, loss, step, val_step, X_val=None, y_val=None):\n",
    "        \n",
    "        im_i = int((im_i - val_step)/step) - 1\n",
    "        \n",
    "        self.history['accuracy'][epoch].append(accuracy)\n",
    "        self.history['loss'][epoch].append(loss)\n",
    "        \n",
    "        val_accuracy, val_loss = self.val_pred(X_val, y_val)\n",
    "        self.history['val_accuracy'][epoch].append(val_accuracy)\n",
    "        self.history['val_loss'][epoch].append(val_loss)\n",
    "        \n",
    "        if ((epoch + 1) * (im_i + 1)) > self.early_stop['patience']:\n",
    "            \n",
    "            if self.history[self.early_stop['monitor']][epoch][-1] >= self.history[self.early_stop['monitor']][self.history['global_max_index'][0]][self.history['global_max_index'][1]]:\n",
    "                self.history['global_max_index'] = (epoch, im_i)\n",
    "            \n",
    "            if self.early_stop['restore_best_layers'] and (epoch, im_i) == self.history['global_max_index']:\n",
    "                self.history['best_layers'] = []\n",
    "                for layer in self.layers:\n",
    "                    if layer.__class__.__name__ == 'Conv':\n",
    "                        self.history['best_layers'].append(layer.filters)\n",
    "                    elif layer.__class__.__name__ == 'MaxPool':\n",
    "                        self.history['best_layers'].append(None)\n",
    "                    elif layer.__class__.__name__ == 'Dense':\n",
    "                        self.history['best_layers'].append((layer.weights, layer.biases))\n",
    "                \n",
    "            min_local_accuracy = min(self.history[self.early_stop['monitor']][epoch][-self.early_stop['patience']:])\n",
    "            difference = abs(min_local_accuracy - self.history[self.early_stop['monitor']][self.history['global_max_index'][0]][self.history['global_max_index'][1]])\n",
    "            \n",
    "            if min_local_accuracy >= self.early_stop['min_monitor'] and difference < self.early_stop['min_delta']:\n",
    "                if self.early_stop['restore_best_layers']:\n",
    "                    for i, layer in enumerate(self.layers):\n",
    "                        if layer.__class__.__name__ == 'Conv':\n",
    "                            layer.filters = self.history['best_layers'][i]\n",
    "                        elif layer.__class__.__name__ == 'MaxPool':\n",
    "                            continue\n",
    "                        elif layer.__class__.__name__ == 'Dense':\n",
    "                            layer.weights, layer.biases = self.history['best_layers'][i]\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def restore_best_layers(self):\n",
    "        if self.early_stop['restore_best_layers']:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if layer.__class__.__name__ == 'Conv':\n",
    "                    layer.filters = self.history['best_layers'][i]\n",
    "                elif layer.__class__.__name__ == 'MaxPool':\n",
    "                    continue\n",
    "                elif layer.__class__.__name__ == 'Dense':\n",
    "                    layer.weights, layer.biases = self.history['best_layers'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = pd.read_csv(\"train.csv\").sample(frac=.05).to_numpy()\n",
    "# X = idx2numpy.convert_from_file('D:\\\\Programming\\\\Projects\\\\data_sets\\\\num_mnist_X.idx3-ubyte').astype('float32')\n",
    "# Y = idx2numpy.convert_from_file('D:\\\\Programming\\\\Projects\\\\data_sets\\\\num_mnist_Y.idx1-ubyte')\n",
    "\n",
    "# X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# # minmax\n",
    "# X = (X - np.min(X, axis=(2,3), keepdims=True)) / (np.max(X, axis=(2,3), keepdims=True) - np.min(X, axis=(2,3), keepdims=True))\n",
    "\n",
    "# # Z-score\n",
    "# # X = (X - np.mean(X, axis=(2,3), keepdims=True)) / np.var(X, axis=(2,3), keepdims=True)\n",
    "\n",
    "# Y_onehot = OneHotEncoder().fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y_onehot, test_size=0.05, random_state=101)\n",
    "\n",
    "# percent = 0.1\n",
    "# sample = np.random.choice([True, False], size=X_test.shape[0], p=[percent, 1-percent])\n",
    "# X_val = X_test[sample]\n",
    "# y_val = y_test[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot',\n",
    "}\n",
    "\n",
    "X_train = idx2numpy.convert_from_file('.\\Fashion_MNIST\\TrainX').astype('float32')\n",
    "y_train = idx2numpy.convert_from_file('.\\Fashion_MNIST\\TrainY').reshape(-1, 1)\n",
    "\n",
    "X_test = idx2numpy.convert_from_file('.\\Fashion_MNIST\\TestX').astype('float32')\n",
    "y_test = idx2numpy.convert_from_file('.\\Fashion_MNIST\\TestY').reshape(-1, 1)\n",
    "\n",
    "X_train, X_test = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# minmax\n",
    "X_max, X_min = np.max(X_train), np.min(X_train)\n",
    "X_train = (X_train - X_min) / (X_max - X_min)\n",
    "X_test = (X_test - X_min) / (X_max - X_min)\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffled_indices]\n",
    "y_train = y_train[shuffled_indices]\n",
    "\n",
    "unique_vals, indices = np.unique(y_train, return_index=True)\n",
    "\n",
    "# Z-score\n",
    "# X_mean, X_std = np.mean(X_train), np.std(X_train), \n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_test = (X_test - X_mean)/X_std\n",
    "\n",
    "sns.countplot(x=y_train.flatten())\n",
    "\n",
    "# One-Hot\n",
    "OneHot = OneHotEncoder(sparse_output=False).fit(y_train)\n",
    "y_train_OH = OneHot.transform(y_train)\n",
    "y_test_OH = OneHot.transform(y_test)\n",
    "\n",
    "percent = 0.01\n",
    "sample = np.random.choice([True, False], size=X_test.shape[0], p=[percent, 1-percent])\n",
    "X_val = X_test[sample]\n",
    "y_val = y_test_OH[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axs[np.unravel_index(i, shape=(2,5))].imshow(X_train[idx, 0], cmap='Greys')\n",
    "    axs[np.unravel_index(i, shape=(2,5))].title.set_text(labels.get(y_train[idx, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Conv(num_filters=5, size=5, stride=1, activation='relu'),\n",
    "    Conv(num_filters=10, size=5, stride=1, activation='relu'), \n",
    "    MaxPool(size=2, stride=2, activation='relu'),\n",
    "    Dense(n_inputs=1000, n_neurons=300, activation='elu'),\n",
    "    Dense(n_inputs=300, n_neurons=10, activation='softmax'),\n",
    "]\n",
    "\n",
    "CNN_model = CNN(layers)\n",
    "\n",
    "CNN_model.EarlyStop(monitor = \"val_accuracy\", min_delta = 1e-2, min_monitor=0.7, patience = 3, restore_best_layers=True)\n",
    "\n",
    "val_step = 10_000\n",
    "step = 5\n",
    "CNN_model.train(X_train, y_train_OH, X_val, y_val, epochs=1, learning_rate=0.01, lr_decay=0.005, step=step, val_step=val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.restore_best_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis = list( range(val_step, val_step + len(CNN_model.history['accuracy'][0])*step, step ) )\n",
    "\n",
    "tck = interpolate.splrep(X_axis, CNN_model.history['accuracy'][0], k = 3, s = 2)\n",
    "accuracy_smoothed = interpolate.splev(X_axis, tck, der = 0)\n",
    "\n",
    "tck = interpolate.splrep(X_axis[:-1], CNN_model.history['val_accuracy'][0], k = 3, s = 0.02)\n",
    "val_accuracy_smoothed = interpolate.splev(X_axis[:-1], tck, der = 0)\n",
    "\n",
    "data = [go.Scatter(x=X_axis, y = accuracy_smoothed, name = 'accuracy'),\n",
    "        go.Scatter(x=X_axis[:-1], y = val_accuracy_smoothed, name = 'val_accuracy')]\n",
    "\n",
    "layout = go.Layout(\n",
    "    height = 1000,\n",
    "    title = dict(\n",
    "        text = 'accuracy per epoch',\n",
    "        font_size = 30,\n",
    "        x = .5\n",
    "    ),\n",
    "    xaxis_title = dict(\n",
    "        text = 'epoch',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    yaxis_title = dict(\n",
    "        text = 'accuracy',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    legend = dict(\n",
    "        x = 0.02, y = .98,\n",
    "        bgcolor = 'rgba(0,0,0,0)',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    margin={'r':40}\n",
    ")\n",
    "go.Figure(data, layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tck = interpolate.splrep(X_axis, CNN_model.history['loss'][0], k = 3, s = 25)\n",
    "loss_smoothed = interpolate.splev(X_axis, tck, der = 0)\n",
    "\n",
    "tck = interpolate.splrep(X_axis[:-1], CNN_model.history['val_loss'][0], k = 3, s = 0.02)\n",
    "val_loss_smoothed = interpolate.splev(X_axis[:-1], tck, der = 0)\n",
    "\n",
    "data = [go.Scatter(x=X_axis, y = loss_smoothed, name = 'loss'),\n",
    "        go.Scatter(x=X_axis[:-1], y = val_loss_smoothed, name = 'val_loss')]\n",
    "\n",
    "layout = go.Layout(\n",
    "    height = 1000,\n",
    "    title = dict(\n",
    "        text = 'loss per epoch',\n",
    "        font_size = 30,\n",
    "        x = .5\n",
    "    ),\n",
    "    xaxis_title = dict(\n",
    "        text = 'epoch',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    yaxis_title = dict(\n",
    "        text = 'loss',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    legend = dict(\n",
    "        x = 0.02, y = .98,\n",
    "        bgcolor = 'rgba(0,0,0,0)',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    margin={'r':0}\n",
    ")\n",
    "go.Figure(data, layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = CNN_model.predict(X_test[:1000])\n",
    "# accuracy_score(np.argmax(y_test[:1000], 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_predicted = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    X_test_class = X_test[y_test.flatten() == i]\n",
    "    predictions = CNN_model.predict(X_test_class)\n",
    "    correctly_predicted.append(np.sum(predictions == i))\n",
    "    accuracies.append(correctly_predicted[-1] / len(predictions))\n",
    "    \n",
    "correctly_predicted, accuracies = np.array(correctly_predicted), np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(x = list(labels.values()), y = correctly_predicted),\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    height = 500,\n",
    "    title = dict(\n",
    "        text = 'Correctly predicted clothes in nums',\n",
    "        font_size = 30,\n",
    "        x = .5\n",
    "    ),\n",
    "    xaxis_title = dict(\n",
    "        text = 'Clothe',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    yaxis_range = [0,1000],\n",
    "    yaxis_title = dict(\n",
    "        text = 'Num of predicted',\n",
    "        font_size = 20\n",
    "    ),\n",
    ")\n",
    "go.Figure(data, layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(x = list(labels.values()), y = accuracies*100),\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    height = 500,\n",
    "    title = dict(\n",
    "        text = 'Accuracy on each clothe type, %',\n",
    "        font_size = 30,\n",
    "        x = .5\n",
    "    ),\n",
    "    xaxis = dict(nticks = 11),\n",
    "    xaxis_title = dict(\n",
    "        text = 'Clothe',\n",
    "        font_size = 20\n",
    "    ),\n",
    "    yaxis_range = [0,100],\n",
    "    yaxis=dict(ticksuffix=\"%\"),\n",
    "    yaxis_title = dict(\n",
    "        text = 'Accuracy, [%]',\n",
    "        font_size = 20\n",
    "    ),\n",
    ")\n",
    "go.Figure(data, layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "university",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
